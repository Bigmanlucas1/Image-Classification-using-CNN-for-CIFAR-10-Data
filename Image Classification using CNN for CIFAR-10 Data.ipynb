{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HABUMUGISHA Emmanuel\n",
    "### 225229109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9 : Image Classification using CNN for CIFAR-10 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6TK3qsiOJzZc"
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JGKQeQEQJ2gE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vEeKKLSNKO17",
    "outputId": "739cdd7c-2062-49ba-ce38-33cd634d3c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n",
    "print('x_train shape:',x_train.shape)\n",
    "print(x_train.shape[0],'train samples')\n",
    "print(x_test.shape[0],'test samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-murREgGLxKd",
    "outputId": "7b515a09-321e-471c-d700-106cb7778600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[5000].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5xyzJY1oMeFQ",
    "outputId": "c86dd7a4-a4bf-45cd-f206-36883175f911",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[444])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "53EoSFK6MxzH",
    "outputId": "20c26737-84c3-4914-f047-f32c97a7890b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2680269c940>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "omLzAwvmOTT4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YbW9O4r9M-iv"
   },
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "y_train=keras.utils.to_categorical (y_train,num_classes)\n",
    "y_test=keras.utils.to_categorical (y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RppsasRpNiwv",
    "outputId": "3e29d78e-241e-4b3a-ec49-b13637e08059"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CzcYoybZPhFE"
   },
   "outputs": [],
   "source": [
    "x_train=x_train.astype('float32')\n",
    "\n",
    "x_test=x_test.astype('float32')\n",
    "x_train /=255\n",
    "x_test /=255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x5fWs7vXTsLP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), strides=(2, 2), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Adam in module keras.src.optimizers.adam:\n",
      "\n",
      "class Adam(keras.src.optimizers.optimizer.Optimizer)\n",
      " |  Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='Adam', **kwargs)\n",
      " |  \n",
      " |  Optimizer that implements the Adam algorithm.\n",
      " |  \n",
      " |  Adam optimization is a stochastic gradient descent method that is based on\n",
      " |  adaptive estimation of first-order and second-order moments.\n",
      " |  \n",
      " |  According to\n",
      " |  [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n",
      " |  the method is \"*computationally\n",
      " |  efficient, has little memory requirement, invariant to diagonal rescaling of\n",
      " |  gradients, and is well suited for problems that are large in terms of\n",
      " |  data/parameters*\".\n",
      " |  \n",
      " |  Args:\n",
      " |    learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n",
      " |      `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
      " |      that takes no arguments and returns the actual value to use. The\n",
      " |      learning rate. Defaults to `0.001`.\n",
      " |    beta_1: A float value or a constant float tensor, or a callable\n",
      " |      that takes no arguments and returns the actual value to use. The\n",
      " |      exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n",
      " |    beta_2: A float value or a constant float tensor, or a callable\n",
      " |      that takes no arguments and returns the actual value to use. The\n",
      " |      exponential decay rate for the 2nd moment estimates. Defaults to\n",
      " |      `0.999`.\n",
      " |    epsilon: A small constant for numerical stability. This epsilon is\n",
      " |      \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
      " |      Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
      " |      `1e-7`.\n",
      " |    amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n",
      " |      the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n",
      " |    name: String. The name to use\n",
      " |        for momentum accumulator weights created by\n",
      " |        the optimizer.\n",
      " |    weight_decay: Float, defaults to None. If set, weight decay is applied.\n",
      " |    clipnorm: Float. If set, the gradient of each weight is individually\n",
      " |        clipped so that its norm is no higher than this value.\n",
      " |    clipvalue: Float. If set, the gradient of each weight is clipped to be no\n",
      " |        higher than this value.\n",
      " |    global_clipnorm: Float. If set, the gradient of all weights is clipped so\n",
      " |        that their global norm is no higher than this value.\n",
      " |    use_ema: Boolean, defaults to False. If True, exponential moving average\n",
      " |        (EMA) is applied. EMA consists of computing an exponential moving\n",
      " |        average of the weights of the model (as the weight values change after\n",
      " |        each training batch), and periodically overwriting the weights with\n",
      " |        their moving average.\n",
      " |    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n",
      " |        This is the momentum to use when computing\n",
      " |        the EMA of the model's weights:\n",
      " |        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n",
      " |        current_variable_value`.\n",
      " |    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n",
      " |        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n",
      " |        we overwrite the model variable by its moving average.\n",
      " |        If None, the optimizer\n",
      " |        does not overwrite model variables in the middle of training, and you\n",
      " |        need to explicitly overwrite the variables at the end of training\n",
      " |        by calling `optimizer.finalize_variable_values()`\n",
      " |        (which updates the model\n",
      " |        variables in-place). When using the built-in `fit()` training loop,\n",
      " |        this happens automatically after the last epoch,\n",
      " |        and you don't need to do anything.\n",
      " |    jit_compile: Boolean, defaults to True.\n",
      " |        If True, the optimizer will use XLA\n",
      " |        compilation. If no GPU device is found, this flag will be ignored.\n",
      " |    mesh: optional `tf.experimental.dtensor.Mesh` instance. When provided,\n",
      " |        the optimizer will be run in DTensor mode, e.g. state\n",
      " |        tracking variable will be a DVariable, and aggregation/reduction will\n",
      " |        happen in the global DTensor context.\n",
      " |    **kwargs: keyword arguments only used for backward compatibility.\n",
      " |  \n",
      " |  Reference:\n",
      " |    - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n",
      " |    - [Reddi et al., 2018](\n",
      " |        https://openreview.net/pdf?id=ryQu7f-RZ) for `amsgrad`.\n",
      " |  \n",
      " |  Notes:\n",
      " |  \n",
      " |  The default value of 1e-7 for epsilon might not be a good default in\n",
      " |  general. For example, when training an Inception network on ImageNet a\n",
      " |  current good choice is 1.0 or 0.1. Note that since Adam uses the\n",
      " |  formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
      " |  the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
      " |  hat\" in the paper.\n",
      " |  \n",
      " |  The sparse implementation of this algorithm (used when the gradient is an\n",
      " |  IndexedSlices object, typically because of `tf.gather` or an embedding\n",
      " |  lookup in the forward pass) does apply momentum to variable slices even if\n",
      " |  they were not used in the forward pass (meaning they have a gradient equal\n",
      " |  to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
      " |  accumulator. This means that the sparse behavior is equivalent to the dense\n",
      " |  behavior (in contrast to some momentum implementations which ignore momentum\n",
      " |  unless a variable slice was actually used).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Adam\n",
      " |      keras.src.optimizers.optimizer.Optimizer\n",
      " |      keras.src.optimizers.optimizer._BaseOptimizer\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, jit_compile=True, name='Adam', **kwargs)\n",
      " |      Create a new Optimizer.\n",
      " |  \n",
      " |  build(self, var_list)\n",
      " |      Initialize optimizer variables.\n",
      " |      \n",
      " |      Adam optimizer has 3 types of variables: momentums, velocities and\n",
      " |      velocity_hat (only set when amsgrad is applied),\n",
      " |      \n",
      " |      Args:\n",
      " |        var_list: list of model variables to build Adam variables on.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the optimizer.\n",
      " |      \n",
      " |      An optimizer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of an optimizer.\n",
      " |      The same optimizer can be reinstantiated later\n",
      " |      (without any saved state) from this configuration.\n",
      " |      \n",
      " |      Subclass optimizer should override this method to include other\n",
      " |      hyperparameters.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  update_step(self, gradient, variable)\n",
      " |      Update step given gradient and the associated model variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.optimizers.optimizer.Optimizer:\n",
      " |  \n",
      " |  add_variable_from_reference(self, model_variable, variable_name, shape=None, initial_value=None)\n",
      " |      Create an optimizer variable from model variable.\n",
      " |      \n",
      " |      Create an optimizer variable based on the information of model variable.\n",
      " |      For example, in SGD optimizer momemtum, for each model variable, a\n",
      " |      corresponding momemtum variable is created of the same shape and dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_variable: tf.Variable. The corresponding model variable to the\n",
      " |          optimizer variable to be created.\n",
      " |        variable_name: String. The name prefix of the optimizer variable to be\n",
      " |          created. The create variables name will follow the pattern\n",
      " |          `{variable_name}/{model_variable.name}`, e.g., `momemtum/dense_1`.\n",
      " |        shape: List or Tuple, defaults to None. The shape of the optimizer\n",
      " |          variable to be created. If None, the created variable will have the\n",
      " |          same shape as `model_variable`.\n",
      " |        initial_value: A Tensor, or Python object convertible to a Tensor,\n",
      " |          defaults to None. The initial value of the optimizer variable, if\n",
      " |          None, the initial value will be default to 0.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An optimizer variable.\n",
      " |  \n",
      " |  aggregate_gradients(self, grads_and_vars)\n",
      " |      Aggregate gradients on all devices.\n",
      " |      \n",
      " |      By default, we will perform reduce_sum of gradients across devices.\n",
      " |      Users can implement their own aggregation logic by overriding this\n",
      " |      method.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of (gradient, variable) pairs.\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, name=None, skip_gradients_aggregation=False, **kwargs)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of `(gradient, variable)` pairs.\n",
      " |        name: string, defaults to None. The name of the namescope to\n",
      " |          use when creating variables. If None, `self.name` will be used.\n",
      " |        skip_gradients_aggregation: If true, gradients aggregation will not be\n",
      " |          performed inside optimizer. Usually this arg is set to True when you\n",
      " |          write custom code aggregating gradients outside the optimizer.\n",
      " |        **kwargs: keyword arguments only used for backward compatibility.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.Variable`, representing the current iteration.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        RuntimeError: If called in a cross-replica context.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  add_variable(self, shape, dtype=None, initializer='zeros', name=None)\n",
      " |      Create an optimizer variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        shape: A list of integers, a tuple of integers, or a 1-D Tensor of\n",
      " |          type int32. Defaults to scalar if unspecified.\n",
      " |        dtype: The DType of the optimizer variable to be created. Defaults to\n",
      " |          `tf.keras.backend.floatx` if unspecified.\n",
      " |        initializer: string or callable. Initializer instance.\n",
      " |        name: The name of the optimizer variable to be created.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An optimizer variable, in the format of tf.Variable.\n",
      " |  \n",
      " |  compute_gradients(self, loss, var_list, tape=None)\n",
      " |      Compute gradients of loss on trainable variables.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: `Tensor` or callable. If a callable, `loss` should take no\n",
      " |          arguments and return the value to minimize.\n",
      " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
      " |          `loss`, or a callable returning the list or tuple of `Variable`\n",
      " |          objects. Use callable when the variable list would otherwise be\n",
      " |          incomplete before `minimize` since the variables are created at the\n",
      " |          first time `loss` is called.\n",
      " |        tape: (Optional) `tf.GradientTape`. If `loss` is provided as a\n",
      " |          `Tensor`, the tape that computed the `loss` must be provided.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of (gradient, variable) pairs. Variable is always present, but\n",
      " |        gradient can be `None`.\n",
      " |  \n",
      " |  exclude_from_weight_decay(self, var_list=None, var_names=None)\n",
      " |      Exclude variables from weight decay.\n",
      " |      \n",
      " |      This method must be called before the optimizer's `build` method is\n",
      " |      called. You can set specific variables to exclude out, or set a list of\n",
      " |      strings as the anchor words, if any of which appear in a variable's\n",
      " |      name, then the variable is excluded.\n",
      " |      \n",
      " |      Args:\n",
      " |          var_list: A list of `tf.Variable`s to exclude from weight decay.\n",
      " |          var_names: A list of strings. If any string in `var_names` appear\n",
      " |              in the model variable's name, then this model variable is\n",
      " |              excluded from weight decay. For example, `var_names=['bias']`\n",
      " |              excludes all bias variables from weight decay.\n",
      " |  \n",
      " |  finalize_variable_values(self, var_list)\n",
      " |      Set the final value of model's trainable variables.\n",
      " |      \n",
      " |      Sometimes there are some extra steps before ending the variable updates,\n",
      " |      such as overriding the model variables with its average value.\n",
      " |      \n",
      " |      Args:\n",
      " |        var_list: list of model variables.\n",
      " |  \n",
      " |  load_own_variables(self, store)\n",
      " |      Set the state of this optimizer object.\n",
      " |  \n",
      " |  minimize(self, loss, var_list, tape=None)\n",
      " |      Minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply computes gradient using `tf.GradientTape` and calls\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: `Tensor` or callable. If a callable, `loss` should take no\n",
      " |          arguments and return the value to minimize.\n",
      " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
      " |          `loss`, or a callable returning the list or tuple of `Variable`\n",
      " |          objects.  Use callable when the variable list would otherwise be\n",
      " |          incomplete before `minimize` since the variables are created at the\n",
      " |          first time `loss` is called.\n",
      " |        tape: (Optional) `tf.GradientTape`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        None\n",
      " |  \n",
      " |  save_own_variables(self, store)\n",
      " |      Get the state of this optimizer object.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Set the weights of the optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          weights: a list of `tf.Variable`s or numpy arrays, the target values\n",
      " |              of optimizer variables. It should have the same order as\n",
      " |              `self._variables`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Creates an optimizer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same optimizer from the config dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of get_config.\n",
      " |          custom_objects: A Python dictionary mapping names to additional\n",
      " |            user-defined Python objects needed to recreate this optimizer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An optimizer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  variables\n",
      " |      Returns variables of this optimizer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.optimizers.optimizer._BaseOptimizer:\n",
      " |  \n",
      " |  iterations\n",
      " |      The number of training steps this `optimizer` has run.\n",
      " |      \n",
      " |      By default, iterations would be incremented by one every time\n",
      " |      `apply_gradients()` is called.\n",
      " |  \n",
      " |  learning_rate\n",
      " |  \n",
      " |  lr\n",
      " |      Alias of `learning_rate()`.\n",
      " |      \n",
      " |      `lr()` is heavily called in workflows using `optimizer_v2.OptimizerV2`,\n",
      " |      so we keep it for backward compabitliy.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.trackable.autotrackable.AutoTrackable:\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(help(tf.keras.optimizers.Adam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8 : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ia8WR0_zU0vX",
    "outputId": "0c1d05bd-e4f3-45a8-c38d-6d9c24090db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 8, 8, 32)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 2, 2, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               66048     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 125002 (488.29 KB)\n",
      "Trainable params: 125002 (488.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.8652 - accuracy: 0.3056 - val_loss: 1.5959 - val_accuracy: 0.4062\n",
      "Epoch 2/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5943 - accuracy: 0.4138 - val_loss: 1.5952 - val_accuracy: 0.4182\n",
      "Epoch 3/15\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4871 - accuracy: 0.4558 - val_loss: 1.4518 - val_accuracy: 0.4642\n",
      "Epoch 4/15\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4255 - accuracy: 0.4788 - val_loss: 1.3353 - val_accuracy: 0.5058\n",
      "Epoch 5/15\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3718 - accuracy: 0.5015 - val_loss: 1.2577 - val_accuracy: 0.5438\n",
      "Epoch 6/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3298 - accuracy: 0.5237 - val_loss: 1.2230 - val_accuracy: 0.5598\n",
      "Epoch 7/15\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.3022 - accuracy: 0.5322 - val_loss: 1.1855 - val_accuracy: 0.5814\n",
      "Epoch 8/15\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2767 - accuracy: 0.5438 - val_loss: 1.2642 - val_accuracy: 0.5476\n",
      "Epoch 9/15\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2564 - accuracy: 0.5547 - val_loss: 1.3083 - val_accuracy: 0.5400\n",
      "Epoch 10/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2330 - accuracy: 0.5646 - val_loss: 1.1098 - val_accuracy: 0.6108\n",
      "Epoch 11/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2183 - accuracy: 0.5715 - val_loss: 1.2023 - val_accuracy: 0.5766\n",
      "Epoch 12/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2080 - accuracy: 0.5740 - val_loss: 1.1322 - val_accuracy: 0.6080\n",
      "Epoch 13/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1917 - accuracy: 0.5807 - val_loss: 1.1663 - val_accuracy: 0.5800\n",
      "Epoch 14/15\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1849 - accuracy: 0.5844 - val_loss: 1.0440 - val_accuracy: 0.6312\n",
      "Epoch 15/15\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1738 - accuracy: 0.5903 - val_loss: 1.0610 - val_accuracy: 0.6220\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.0903 - accuracy: 0.6201\n",
      "Test accuracy: 0.6201000213623047\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with RMSprop optimizer and categorical_crossentropy loss\n",
    "from keras.optimizers import RMSprop\n",
    "optimizer =tf.keras.optimizers.legacy.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part -II Model Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CsHiuL2VWUSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               8389120   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8782666 (33.50 MB)\n",
      "Trainable params: 8782666 (33.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Total Parameters: 8782666\n",
      "Epoch 1/5\n",
      "704/704 [==============================] - 347s 491ms/step - loss: 1.4756 - accuracy: 0.4732 - val_loss: 1.2042 - val_accuracy: 0.5512\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 349s 496ms/step - loss: 0.9249 - accuracy: 0.6767 - val_loss: 0.9552 - val_accuracy: 0.6662\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 373s 530ms/step - loss: 0.6821 - accuracy: 0.7643 - val_loss: 1.1602 - val_accuracy: 0.6126\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 369s 524ms/step - loss: 0.5010 - accuracy: 0.8267 - val_loss: 1.0734 - val_accuracy: 0.6558\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 367s 521ms/step - loss: 0.3460 - accuracy: 0.8810 - val_loss: 0.7470 - val_accuracy: 0.7682\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.8069 - accuracy: 0.7452\n",
      "Test accuracy: 0.745199978351593\n"
     ]
    }
   ],
   "source": [
    "# Define the more complicated CNN architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary and parameter count\n",
    "model.summary()\n",
    "print(\"Total Parameters:\", model.count_params())\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-Ow69SqX09Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
